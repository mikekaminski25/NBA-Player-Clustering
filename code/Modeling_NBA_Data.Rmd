---
title: "NBA Clustering Project - Tidymodels"
author: "Mike Kaminski"
date: "2023-06-08"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, cache = TRUE, warning = FALSE,
                      message = FALSE, dpi = 180,
                      fig.width = 10, fig.height = 5)

knitr::opts_knit$set(root.dir = "C:/Users/mikek/Documents/NBA-Player-Clustering")
```

```{r}
library(tidymodels) # broom, dials, parsnip, tune, workflows, yardstick
library(tidyverse) #ggplot2, dplyr, tidyr, readr, purr, tibble, stringr, lubridate
library(tidytext)
library(knitr)
library(stringi)
library(corrr)
library(tidyclust)
```


# Modeling


```{r}
NBACleanData <- read_csv("Data/model0611.csv",show_col_types = FALSE) %>% select(-1)
model_df <-  NBACleanData
```

## PCA
The data is fairly high dimensional, so I'll try reducing the dimensionality using PCA.

### Make a recipe
```{r}
nba_rec <- recipe(~ ., data = model_df) %>%
  update_role(Year, Player, Pos, Tm, new_role = "id") %>% #might need these variables later
  step_normalize(all_predictors()) %>%
  step_pca(all_predictors(), num_comp = 20) #default for PCs is 5

nba_prep <- prep(nba_rec)
```

### PC - Tidied and plotted
```{r fig.width = 15, fig.height=6}
tidied_pca <- tidy(nba_prep, 2) # 2 refers to the step number.  This provides the step_pca

#shows just a few PCs
tidied_pca %>%
  filter(component %in% paste0("PC", 1:9)) %>%
  group_by(component) %>%
  top_n(10, abs(value)) %>%
  ungroup() %>%
  mutate(terms = reorder_within(terms, abs(value), component)) %>%
  ggplot(aes(abs(value), terms, fill = value > 0)) +
  geom_col() +
  facet_wrap(~component, scales = "free_y") +
  scale_y_reordered() +
  labs(
    x = "Absolute value of contribution",
    y = NULL, fill = "Positive?"
  )

```

### How much variance is being captured?
```{r, fig.width= 14, fig.height = 3}

sdev <- nba_prep$steps[[2]]$res$sdev 

percent_variation <- sdev^2 / sum(sdev^2)

tibble(component = unique(tidied_pca$component),
       percent_var = percent_variation) %>%
  filter(component %in% paste0("PC", 1:20)) %>%
  mutate(component = fct_inorder(component)) %>%
  ggplot(aes(component, percent_var)) +
  geom_col(position = "stack") +
  geom_text(aes(label = scales::percent((round(percent_var,4))),
            vjust = -0.5), size = 3, color = "red"
            ) +
  geom_text(aes(label = scales::percent(cumsum(round(percent_var,4))),
            vjust = -2), size = 4
            ) +
  scale_y_continuous(labels = scales::percent_format(), limits = c(0,.35))
```
- The first 2 PCs capture about 50% of the variance, top 3 capture 60%, top 5 capture 70%, top 9 capture 80%, top 16 capture 90%. 
- 16 PCs have a value above 1, so could potentially consider those

Capturing 80% of the variance might be a good place to start

### Recipe that captures 80% variance
```{r}
nba_rec_80 <- recipe(~ ., data = model_df) %>%
  update_role(Year, Player, Pos, Tm, new_role = "id") %>%
  step_normalize(all_predictors()) %>%
  step_pca(all_predictors(), num_comp = 9)

nba_prep_80 <- prep(nba_rec_80)
```

Start with 3 clusters just to see how the data looks
```{r}
# the template field stores the PCA variables
pca_80 <- nba_prep_80$template %>%
    mutate(Year = as.character(Year),
         Player = as.character(Player),
         Pos = as.character(Pos),
         Tm = as.character(Tm)) %>%
  as_tibble()

set.seed(525)

kclust <- kmeans(pca_80 %>% select(-c(1:4)), centers = 3)
```

### Tidy the kclust data
```{r}
tidy(kclust)
```

### Augment and plot the kclust data
```{r}
augment(kclust, pca_80) %>%
  ggplot(aes(PC1, PC1, color = .cluster)) + # the PCx values can be changed as needed
  geom_point()
```

### Glance the kclust data
```{r}
glance(kclust)
```

### Find the Optimal Number of k
```{r}
kclusts <- 
  tibble(k = 1:15) %>%
  mutate(
    kclust = map(k, ~ kmeans(pca_80 %>% select(-c(1:4)), .x)),
    tidied = map(kclust, tidy),
    glanced = map(kclust, glance),
    augmented = map(kclust, augment, pca_80 %>% select(-c(1:4)))
  )

```

#### Elbow plots
```{r fig.width = 10, fig.height=3}
kclusts %>%
  unnest(glanced) %>%
  ggplot(aes(k, tot.withinss)) +
  geom_line(alpha = 0.8) +
  geom_point(size = 2)

```
The elbow looks like it's at 3.  It may be beneficial to plot the change for each k from k to k+1, k+2, and k+3

#### Elbow plots - k+1, k+2, k+3
This shows the difference from k to k+1, k to k+2, and k to k+3
```{r}
kclusts %>%
  unnest(glanced) %>%
  ggplot(aes(k, tot.withinss)) +
  # geom_line(alpha = 0.8) +
  # geom_point(size = 2) +

  #change from k = 1 to k = 2
  geom_line(aes(k, tot.withinss-lead(tot.withinss), color = "k+1"), alpha = 0.8 ) +
  geom_point(aes(k, tot.withinss-lead(tot.withinss)),size = 2,color = "red") +

  #change from k = 1 to k = 3
  geom_line(aes(k, tot.withinss-lead(tot.withinss,n=2),color = "k+2"), alpha = 0.8) +
  geom_point(aes(k, tot.withinss-lead(tot.withinss,n=2)),size = 2, color = "green") +

  #change from k = 1 to k = 4
  geom_line(aes(k, tot.withinss-lead(tot.withinss,n=3),color = "k+3"), alpha = 0.8) +
  geom_point(aes(k, tot.withinss-lead(tot.withinss,n=3)),size = 2, color = "blue") +
  
  labs(
    x = "Number of Clusters (k)",
    y = "tot.withinss", color = "",
    title = "Where does the data level off?"
  )
```
It's a bit difficult to determine the optimal number of k.  An argument could be made for 3 (similar to the above just using k), but it seems unreasonable to have just 3 positions for NBA players. It might be be good to have a team with players that all fall in the "best" cluster, but more analysis would have to be done on historically good teams.

An argument could be made for 6 or 8 when looking at the difference in tot.withinss from k to k+1/k+2/k+3 - it levels off around there.

Filtering down the data to include less players might make things better.

### Try with less data
From the initial cleaning, players with mpg of less than 12 minutes per game were removed.  Removing players with less than 20 mins per game could make things more clear.

#### Recipe
```{r}
model_df_mpg20 <-  NBACleanData %>%
  filter(MP_pg >= 20)

nba_rec_mpg20 <- recipe(~ ., data = model_df_mpg20) %>%
  update_role(Year, Player, Pos, Tm, new_role = "id") %>%
  step_normalize(all_predictors()) %>%
  step_pca(all_predictors())

nba_prep_mpg20 <- prep(nba_rec_mpg20)

tidied_pca_mpg20 <- tidy(nba_prep_mpg20, 2)

```

### How much variance is being captured?
```{r, fig.width= 14, fig.height = 3}
sdev <- nba_prep_mpg20$steps[[2]]$res$sdev

percent_variation <- sdev^2 / sum(sdev^2)

tibble(component = unique(tidied_pca_mpg20$component),
       percent_var = percent_variation) %>%
  filter(component %in% paste0("PC", 1:20)) %>%
  mutate(component = fct_inorder(component)) %>%
  ggplot(aes(component, percent_var)) +
  geom_col(position = "stack") +
  geom_text(aes(label = scales::percent((round(percent_var,4))),
            vjust = -0.5), size = 3, color = "red"
            ) +
  geom_text(aes(label = scales::percent(cumsum(round(percent_var,4))),
            vjust = -2), size = 4
            ) +
  scale_y_continuous(labels = scales::percent_format(), limits = c(0,.35))
```

Try with 8 PCs, which captures 80% of the variation
```{r}
nba_rec_mpg20 <- recipe(~ ., data = model_df_mpg20) %>%
  update_role(Year, Player, Pos, Tm, new_role = "id") %>%
  step_normalize(all_predictors()) %>%
  step_pca(all_predictors(), num_comp = 8)

nba_prep_mpg20 <- prep(nba_rec_mpg20)
```

```{r}
# the template stores the PCA variables
pca_80 <- nba_prep_mpg20$template %>%
    mutate(Year = as.character(Year),
         Player = as.character(Player),
         Pos = as.character(Pos),
         Tm = as.character(Tm)) %>%
  as_tibble()

set.seed(525)

kclust <- kmeans(pca_80 %>% select(-c(1:4)), centers = 3)
```

```{r}
kclusts <- 
  tibble(k = 1:15) %>%
  mutate(
    kclust = map(k, ~ kmeans(pca_80 %>% select(-c(1:4)), .x)),
    tidied = map(kclust, tidy),
    glanced = map(kclust, glance),
    augmented = map(kclust, augment, pca_80 %>% select(-c(1:4)))
  )
```

#### Elbow plots
```{r fig.width = 10, fig.height=3}
kclusts %>%
  unnest(glanced) %>%
  ggplot(aes(k, tot.withinss)) +
  geom_line(alpha = 0.8) +
  geom_point(size = 2)

```
The elbow looks like it's at 3.  It may be beneficial to plot the change for each k from k to k+1, k+2, and k+3

#### Elbow plots - k+1, k+2, k+3
This shows the difference from k to k+1, k to k+2, and k to k+3
```{r}
kclusts %>%
  unnest(glanced) %>%
  ggplot(aes(k, tot.withinss)) +

  #change from k = 1 to k = 2
  geom_line(aes(k, tot.withinss-lead(tot.withinss), color = "k+1"), alpha = 0.8 ) +
  geom_point(aes(k, tot.withinss-lead(tot.withinss)),size = 2,color = "red") +

  #change from k = 1 to k = 3
  geom_line(aes(k, tot.withinss-lead(tot.withinss,n=2),color = "k+2"), alpha = 0.8) +
  geom_point(aes(k, tot.withinss-lead(tot.withinss,n=2)),size = 2, color = "green") +

  #change from k = 1 to k = 4
  geom_line(aes(k, tot.withinss-lead(tot.withinss,n=3),color = "k+3"), alpha = 0.8) +
  geom_point(aes(k, tot.withinss-lead(tot.withinss,n=3)),size = 2, color = "blue") +
  
  labs(
    x = "Number of Clusters (k)",
    y = "tot.withinss", color = "",
    title = "Where does the data level off?"
  )
```
That really didn't help, still looks like 3 might be optimal, 6 or 8 could work though.

```{r include =FALSE}

### try with removing advanced stats and a few other stats that are highly correlated
# Advanced stats are often a formula/combination of the other underlying stats included in the model.  Additionally, points likely impacts FG, FT, ThrPa, TwoPA, and other similar stats. Attempts and makes are also strongly correlated

# model_df_var_adj <-  NBACleanData %>%
#   select(-c(
#     contains("_adv"),
#     FG_pp, FG_pg, #remove makes, but keep attempts
#     FT_pp, FT_pg, #remove makes, but keep attempts
#     ThrP_pp, ThrP_pg, #remove makes, but keep attempts
#     TwoP_pp, TwoP_pg, #remove makes, but keep attempts
#     Reb_T_pg, Reb_T_pp, # remove total rebounds, keep offensive and defensive
#     HOF # I just don't want it
#     )
#   )
# 
# nba_rec_pre_var_adj <- recipe(~ ., data = model_df_var_adj) %>%
#   update_role(Year, Player, Pos, Tm, new_role = "id") %>%
#   step_normalize(all_predictors()) %>%
#   step_pca(all_predictors())
# 
# nba_prep_pre_var_adj <- prep(nba_rec_pre_var_adj)
# 
# tidied_pca_pre_var_adj <- tidy(nba_prep_pre_var_adj, 2)

```

```{r, fig.width= 14, fig.height = 3,nclude =FALSE}
# Check to see how much variance is being captured
# sdev <- nba_prep_pre_var_adj$steps[[2]]$res$sdev
# 
# percent_variation <- sdev^2 / sum(sdev^2)
# 
# tibble(component = unique(tidied_pca_pre_var_adj$component),
#        percent_var = percent_variation) %>%
#   filter(component %in% paste0("PC", 1:20)) %>%
#   mutate(component = fct_inorder(component)) %>%
#   ggplot(aes(component, percent_var)) +
#   geom_col(position = "stack") +
#     geom_text(aes(label = scales::percent((round(percent_var,4))),
#             vjust = -0.5)
#             ) +
#   geom_text(aes(label = scales::percent(cumsum(percent_var)),
#             vjust = -1.5)
#             ) +
#   scale_y_continuous(labels = scales::percent_format(), limits = c(0,.35))
```

```{r include=FALSE}
# 
# I can try with 10 PCs
# nba_rec_pre80_var_adj <- recipe(~ ., data = model_df_var_adj) %>%
#   update_role(Year, Player, Pos, Tm, new_role = "id") %>%
#   step_normalize(all_predictors()) %>%
#   step_pca(all_predictors(), num_comp = 8)
# 
# nba_prep_pre80_var_adj <- prep(nba_rec_pre80_var_adj)
```

```{r include=FALSE}
# # the template stores the PCA variables
# pca_80_var_adj <- nba_prep_pre80_var_adj$template %>%
#     mutate(Year = as.character(Year),
#          Player = as.character(Player),
#          Pos = as.character(Pos),
#          Tm = as.character(Tm)) %>%
#   as_tibble()
# 
# set.seed(525)
# 
# kclust <- kmeans(pca_80_var_adj %>% select(-c(1:4)), centers = 3)
```


```{r include=FALSE}
# kclusts <- 
#   tibble(k = 1:15) %>%
#   mutate(
#     kclust = map(k, ~ kmeans(pca_80_var_adj %>% select(-c(1:4)), .x)),
#     tidied = map(kclust, tidy),
#     glanced = map(kclust, glance),
#     augmented = map(kclust, augment, pca_80_var_adj %>% select(-c(1:4)))
#   )

```

```{r, include=FALSE}
# kclusts %>%
#   unnest(glanced) %>%
#   ggplot(aes(k, tot.withinss)) +
#   geom_line(alpha = 0.8) +
#   geom_point(size = 2)
# 
# kclusts %>%
#   unnest(glanced) %>%
#   ggplot(aes(k, tot.withinss-lead(tot.withinss))) +
#   geom_line(alpha = 0.8) +
#   geom_point(size = 2)
# 
# kclusts %>%
#   unnest(glanced) %>%
#   ggplot(aes(k, tot.withinss-lead(tot.withinss,n=2))) +
#   geom_line(alpha = 0.8) +
#   geom_point(size = 2)
# 
# 
# kclusts %>%
#   unnest(glanced) %>%
#   ggplot(aes(k, tot.withinss)) +
#   # geom_line(alpha = 0.8) +
#   # geom_point(size = 2) +
# 
#   geom_line(aes(k, tot.withinss-lead(tot.withinss)), alpha = 0.8 ) +
#   geom_point(aes(k, tot.withinss-lead(tot.withinss)),size = 2,color = "red") +
# 
#   geom_line(aes(k, tot.withinss-lead(tot.withinss,n=2)), alpha = 0.8) +
#   geom_point(aes(k, tot.withinss-lead(tot.withinss,n=2)),size = 2, color = "blue") +
# 
#   geom_line(aes(k, tot.withinss-lead(tot.withinss,n=3)), alpha = 0.8) +
#   geom_point(aes(k, tot.withinss-lead(tot.withinss,n=3)),size = 2, color = "green")
```

### Try with just stats from each category
There were 4 stat categories used in the analysis.  Maybe running PCA on each of them serparately will provide clearer results

###Advanced
```{r}
model_df_adv <-  NBACleanData %>%
  select(Year, Player, Pos, Tm,contains("_adv"))

nba_rec_adv <- recipe(~ ., data = model_df_adv) %>%
  update_role(Year, Player, Pos, Tm, new_role = "id") %>%
  step_normalize(all_predictors()) %>%
  step_pca(all_predictors())

nba_prep_adv <- prep(nba_rec_adv)

tidied_pca_adv <- tidy(nba_prep_adv, 2)
```

Check to see how much variance is being captured
```{r, fig.width= 14, fig.height = 3}
sdev <- nba_prep_adv$steps[[2]]$res$sdev

percent_variation <- sdev^2 / sum(sdev^2)

tibble(component = unique(tidied_pca_adv$component),
       percent_var = percent_variation) %>%
  filter(component %in% paste0("PC", 1:10)) %>%
  mutate(component = fct_inorder(component)) %>%
  ggplot(aes(component, percent_var)) +
  geom_col(position = "stack") +
  geom_text(aes(label = scales::percent((round(percent_var,4))),
            vjust = -0.5), size = 3, color = "red"
            ) +
  geom_text(aes(label = scales::percent(cumsum(round(percent_var,4))),
            vjust = -2), size = 4
            ) +
  scale_y_continuous(labels = scales::percent_format(), limits = c(0,.50))
```
3 70%, 4 80%, 7 90%

###Per 100 possessions
```{r}
model_df_pp <-  NBACleanData %>%
  select(Year, Player, Pos, Tm,contains("_pp"))

nba_rec_ppp <- recipe(~ ., data = model_df_pp) %>%
  update_role(Year, Player, Pos, Tm, new_role = "id") %>%
  step_normalize(all_predictors()) %>%
  step_pca(all_predictors())

nba_prep_pp <- prep(nba_rec_ppp)

tidied_pca_pp <- tidy(nba_prep_pp, 2)
```

Check to see how much variance is being captured
```{r, fig.width= 14, fig.height = 3}
sdev <- nba_prep_pp$steps[[2]]$res$sdev

percent_variation <- sdev^2 / sum(sdev^2)

tibble(component = unique(tidied_pca_pp$component),
       percent_var = percent_variation) %>%
  filter(component %in% paste0("PC", 1:10)) %>%
  mutate(component = fct_inorder(component)) %>%
  ggplot(aes(component, percent_var)) +
  geom_col(position = "stack") +
  geom_text(aes(label = scales::percent((round(percent_var,4))),
            vjust = -0.5), size = 3, color = "red"
            ) +
  geom_text(aes(label = scales::percent(cumsum(round(percent_var,4))),
            vjust = -2), size = 4
            ) +
  scale_y_continuous(labels = scales::percent_format(), limits = c(0,.50))
```
4 70%, 6 80%, 9 90%

### Shooting
```{r}
model_df_sho <-  NBACleanData %>%
  select(Year, Player, Pos, Tm,contains("_sho"))

nba_rec_sho <- recipe(~ ., data = model_df_sho) %>%
  update_role(Year, Player, Pos, Tm, new_role = "id") %>%
  step_normalize(all_predictors()) %>%
  step_pca(all_predictors())

nba_prep_sho <- prep(nba_rec_sho)

tidied_pca_psho <- tidy(nba_prep_sho, 2)
```

Check to see how much variance is being captured
```{r, fig.width= 14, fig.height = 3}
sdev <- nba_prep_sho$steps[[2]]$res$sdev

percent_variation <- sdev^2 / sum(sdev^2)

tibble(component = unique(tidied_pca_psho$component),
       percent_var = percent_variation) %>%
  filter(component %in% paste0("PC", 1:10)) %>%
  mutate(component = fct_inorder(component)) %>%
  ggplot(aes(component, percent_var)) +
  geom_col(position = "stack") +
  geom_text(aes(label = scales::percent((round(percent_var,4))),
            vjust = -0.5), size = 3, color = "red"
            ) +
  geom_text(aes(label = scales::percent(cumsum(round(percent_var,4))),
            vjust = -2), size = 4
            ) +
  scale_y_continuous(labels = scales::percent_format(), limits = c(0,.50))
```
5 70% 7 80% 9 90%

###Per Game
```{r}
model_df_pg <-  NBACleanData %>%
  select(Year, Player, Pos, Tm,contains("_pg"))

nba_rec_pg <- recipe(~ ., data = model_df_pg) %>%
  update_role(Year, Player, Pos, Tm, new_role = "id") %>%
  step_normalize(all_predictors()) %>%
  step_pca(all_predictors())

nba_prep_pg <- prep(nba_rec_pg)

tidied_pca_pg <- tidy(nba_prep_pg, 2)
```

Check to see how much variance is being captured
```{r, fig.width= 14, fig.height = 3}
sdev <- nba_prep_pg$steps[[2]]$res$sdev

percent_variation <- sdev^2 / sum(sdev^2)

tibble(component = unique(tidied_pca_pg$component),
       percent_var = percent_variation) %>%
  filter(component %in% paste0("PC", 1:10)) %>%
  mutate(component = fct_inorder(component)) %>%
  ggplot(aes(component, percent_var)) +
  geom_col(position = "stack") +
  geom_text(aes(label = scales::percent((round(percent_var,4))),
            vjust = -0.5), size = 3, color = "red"
            ) +
  geom_text(aes(label = scales::percent(cumsum(round(percent_var,4))),
            vjust = -2), size = 4
            ) +
  scale_y_continuous(labels = scales::percent_format(), limits = c(0,.65))
```
2 70%, 3 80%, 6 90%

Per game stats give us the best values, capturing 72% of the variance in the first 2 PCs

```{r}
# the template stores the PCA variables
pca_pg <- nba_prep_pg$template %>%
    mutate(Year = as.character(Year),
         Player = as.character(Player),
         Pos = as.character(Pos),
         Tm = as.character(Tm)) %>%
  as_tibble()

set.seed(525)

kclust <- kmeans(pca_pg %>% select(-c(1:4)), centers = 3, nstart = 1000, iter.max = 10000)

```


```{r}
kclusts <- 
  tibble(k = 1:15) %>%
  mutate(
    kclust = map(k, ~ kmeans(pca_pg %>% select(-c(1:4)), .x)),
    tidied = map(kclust, tidy),
    glanced = map(kclust, glance),
    augmented = map(kclust, augment, pca_pg %>% select(-c(1:4)))
  )

```
#### Elbow plots
```{r fig.width = 10, fig.height=3}
kclusts %>%
  unnest(glanced) %>%
  ggplot(aes(k, tot.withinss)) +
  geom_line(alpha = 0.8) +
  geom_point(size = 2)

```
The elbow looks like it's at 2. It may be beneficial to plot the change for each k from k to k+1, k+2, and k+3

#### Elbow plots - k+1, k+2, k+3
This shows the difference from k to k+1, k to k+2, and k to k+3
```{r}
kclusts %>%
  unnest(glanced) %>%
  ggplot(aes(k, tot.withinss)) +

  #change from k = 1 to k = 2
  geom_line(aes(k, tot.withinss-lead(tot.withinss), color = "k+1"), alpha = 0.8 ) +
  geom_point(aes(k, tot.withinss-lead(tot.withinss)),size = 2,color = "red") +

  #change from k = 1 to k = 3
  geom_line(aes(k, tot.withinss-lead(tot.withinss,n=2),color = "k+2"), alpha = 0.8) +
  geom_point(aes(k, tot.withinss-lead(tot.withinss,n=2)),size = 2, color = "green") +

  #change from k = 1 to k = 4
  geom_line(aes(k, tot.withinss-lead(tot.withinss,n=3),color = "k+3"), alpha = 0.8) +
  geom_point(aes(k, tot.withinss-lead(tot.withinss,n=3)),size = 2, color = "blue") +
  
  labs(
    x = "Number of Clusters (k)",
    y = "tot.withinss", color = "",
    title = "Where does the data level off?"
  )
```


# Hierarchical

- High dimensional data may run slowly in Hierarchical clustering, so I'll use PCA
- 9 PCs make up 80% of the variance of the entire dataset, so I'll start with that.  It could still run pretty slow, but we'll see.

## All Data
```{r fig.height = 4}
# recipe
nba_rec_hier <- recipe(~ ., data = model_df) %>%
  update_role(Year, Player, Pos, Tm, new_role = "id") %>%
  step_normalize(all_predictors()) %>%
  step_pca(all_predictors(), num_comp = 9)

#prep -  needed for tidied_pca and plot
nba_prep_hier <- prep(nba_rec_hier)

tidied_pca_hier <- tidy(nba_prep_hier,2)

sdev <- nba_prep_hier$steps[[2]]$res$sdev

percent_variation <- sdev^2 / sum(sdev^2)

tibble(component = unique(tidied_pca_hier$component),
       percent_var = percent_variation) %>%
  filter(component %in% paste0("PC", 1:9)) %>%
  mutate(component = fct_inorder(component)) %>%
  ggplot(aes(component, percent_var)) +
  geom_col(position = "stack") +
  geom_text(aes(label = scales::percent((round(percent_var,4))),
            vjust = -0.5), size = 3, color = "red"
            ) +
  geom_text(aes(label = scales::percent(cumsum(round(percent_var,4))),
            vjust = -2), size = 4
            ) +
  scale_y_continuous(labels = scales::percent_format(), limits = c(0,.35))

# new dataframe
nba_bake_hier <- bake(nba_prep_hier, new_data = NULL) %>%
  mutate(across(where(is.factor), as.character)) 


```

#### Try the 3 different clustering methods to check for balance
```{r}
# the dendrograms take too long to generate if all the data is included, so a sample is taken
model_df_hc <- nba_bake_hier %>%
  sample_n(1000) %>%
  select(-c(1:4)) 

set.seed(42069)

# Distance between observations matrix
d <- dist(x=model_df_hc, method = "euclidean")

# Hierarchical clustering using Complete Linkage
nba_hclust_complete <- hclust(d, method = "complete")

# Hierarchical clustering using Average Linkage
nba_hclust_average <- hclust(d, method = "average")

# Hierarchical clustering using Ward Linkage
nba_hclust_ward <- hclust(d, method = "ward.D2")

```

#### Dendrogram of each
```{r}
library(factoextra)

fviz_dend(nba_hclust_complete, main = "Complete")
fviz_dend(nba_hclust_average, main = "Average Linkage")
fviz_dend(nba_hclust_ward, main = "Ward")

```

The below looks at the agglomerative coefficent, which measures clustering structure of the dataset.  A value closer to 1 means more balanced.

```{r}
library(cluster)

ac_metric <- list(
  complete_ac = agnes(model_df_hc, metric = "euclidean", method = "complete")$ac,
  average_ac = agnes(model_df_hc, metric = "euclidean", method = "average")$ac,
  ward_ac = agnes(model_df_hc, metric = "euclidean", method = "ward")$ac
)

ac_metric
```
Ward's is the closest to one, so that's the method that we'll use.

I can run with all the data and omit the dendrograms
```{r}
# the dendrograms take too long to generate if all the data is included, so a sample is taken
model_df_hc <- nba_bake_hier %>%
  select(-c(1:4)) 

set.seed(42069)

# Distance between observations matrix
d <- dist(x=model_df_hc, method = "euclidean")

# Hierarchical clustering using Complete Linkage
nba_hclust_complete <- hclust(d, method = "complete")

# Hierarchical clustering using Average Linkage
nba_hclust_average <- hclust(d, method = "average")

# Hierarchical clustering using Ward Linkage
nba_hclust_ward <- hclust(d, method = "ward.D2")

ac_metric <- list(
  complete_ac = agnes(model_df_hc, metric = "euclidean", method = "complete")$ac,
  average_ac = agnes(model_df_hc, metric = "euclidean", method = "average")$ac,
  ward_ac = agnes(model_df_hc, metric = "euclidean", method = "ward")$ac
)

ac_metric
```

This further supports using wards

This will give us the optimal number of clusters.  Similar to the methods used earlier in the analysis with k-means
```{r}
fviz_nbclust(model_df_hc, FUNcluster = hcut, method = "wss", k.max = 15)

```
Again, this gives us 3, even in a lower dimensional space.

