---
title: "Reclassifying NBA Player Postions - Clustering Analysis Using Tidymodels"
author: "Mike Kaminski"
date: "2023-06-08"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, cache = TRUE, warning = FALSE,
                      message = FALSE, dpi = 180,
                      fig.width = 10, fig.height = 5)

knitr::opts_knit$set(root.dir = "C:/Users/mikek/Documents/NBA-Player-Clustering")
```

# Intro
I've scraped data from https://www.basketball-reference.com/ for 4 stat categories for all seasons from 2001 - 2022 <br>
- Advanced <br>
- Per Game <br>
- Per 100 Possession <br>
- Shooting <br>

I removed players who averaged less than 12 minutes per game during any given season.  I may potentially remove players who played less than 20 mins per game.

# Libraries and Loading Data
```{r}
library(tidymodels) # broom, dials, parsnip, tune, workflows, yardstick
library(tidyverse) #ggplot2, dplyr, tidyr, readr, purr, tibble, stringr, lubridate
library(tidytext)
library(knitr)
library(stringi)
library(corrr)
library(tidyclust)
```

```{r}
NBACleanData <- read_csv("Data/model0611.csv",show_col_types = FALSE) %>% select(-1)
model_df <-  NBACleanData
```

# K-Means
```{r}
nba_clust <- kmeans(model_df %>% select(-c(1:4,80)), centers = 3) # removes und needed variables
tidy(nba_clust) # centers of the clusters
augment(nba_clust, model_df) # shows all the data and the cluster number

```

### Tidy the nba_clust data
```{r}
tidy(nba_clust)[, 1:10] # centers of the clusters
```

### Augment and plot the nba_clust data
```{r}
#augment shows all the data and includes the cluster number
head(augment(nba_clust, model_df)[, c(81,1:9)],5) 
```

```{r}
augment(nba_clust, model_df) %>%
  ggplot(aes(FG_pp, FGA_pp, color = .cluster)) + # the aes values can be changed if needed
  geom_point()
```

### Glance the nba_clust data
```{r}
#glance gives us other useful metrics from the model
glance(nba_clust)
```
### Elbow Plot: Finding the optimal number of k
```{r fig.width = 10, fig.height=3}
kclusts <- 
  tibble(k = 1:15) %>%
  mutate(
    kclust = map(k, ~ kmeans(model_df %>% select(-c(1:4)), .x)),
    tidied = map(kclust, tidy),
    glanced = map(kclust, glance),
    augmented = map(kclust, augment, model_df %>% select(-c(1:4)))
  )

kclusts %>%
  unnest(glanced) %>%
  ggplot(aes(k, tot.withinss)) +
  geom_line(alpha = 0.8) +
  geom_point(size = 2)

```
The elbow looks like it's at 2 or 3.  It may be beneficial to plot the change for each k from k to k+1, k+2, and k+3

### Elbow plots - k+1, k+2, k+3
This shows the difference from k to k+1, k to k+2, and k to k+3
```{r}
kclusts %>%
  unnest(glanced) %>%
  ggplot(aes(k, tot.withinss)) +

  #change from k = 1 to k = 2
  geom_line(aes(k, tot.withinss-lead(tot.withinss), color = "k+1"), alpha = 0.8 ) +
  geom_point(aes(k, tot.withinss-lead(tot.withinss)),size = 2,color = "red") +

  #change from k = 1 to k = 3
  geom_line(aes(k, tot.withinss-lead(tot.withinss,n=2),color = "k+2"), alpha = 0.8) +
  geom_point(aes(k, tot.withinss-lead(tot.withinss,n=2)),size = 2, color = "green") +

  #change from k = 1 to k = 4
  geom_line(aes(k, tot.withinss-lead(tot.withinss,n=3),color = "k+3"), alpha = 0.8) +
  geom_point(aes(k, tot.withinss-lead(tot.withinss,n=3)),size = 2, color = "blue") +
  
  labs(
    x = "Number of Clusters (k)",
    y = "tot.withinss", color = "",
    title = "Where does the data level off?"
  )
```
It's a bit difficult to determine the optimal number of k.  An argument could be made for 3, but it seems unreasonable to have just 3 positions for NBA players. It could be beneficial to have a team with players that all fall in the "best" cluster, but more analysis would have to be done on historically good teams.

An argument could be made for 6 or 7 when looking at the difference in tot.withinss from k to k+1/k+2/k+3 - it levels off around there.

The data is fairly high dimensional, so maybe reducing the dimensionality will yield more interpretable results.

# PCA and K-Means
Reducing the dimensionality using PCA.

### Create a recipe using all the data
```{r}
#create a recipe
nba_rec <- recipe(~ ., data = model_df) %>%
  update_role(Year, Player, Pos, Tm, new_role = "id") %>% #might need these variables later
  step_normalize(all_predictors()) %>% #normalizes the data
  step_pca(all_predictors(), num_comp = 20) #default for PCs is 5

#prep the data
nba_prep <- prep(nba_rec)
```

### PCs - Tidied and Plotted
```{r fig.width = 15, fig.height=6}
#shows just a few PCs
tidied_pca <- tidy(nba_prep, 2) # 2 refers to the step number. This provides the step_pca

# Plots the first few PCs a
tidied_pca %>%
  filter(component %in% paste0("PC", 1:9)) %>%
  group_by(component) %>%
  top_n(10, abs(value)) %>%
  ungroup() %>%
  mutate(terms = reorder_within(terms, abs(value), component)) %>%
  ggplot(aes(abs(value), terms, fill = value > 0)) +
  geom_col() +
  facet_wrap(~component, scales = "free_y") +
  scale_y_reordered() +
  labs(
    x = "Absolute value of contribution",
    y = NULL, fill = "Positive?"
  )

```

### How much variance is being captured?
```{r, fig.width= 14, fig.height = 3}

sdev <- nba_prep$steps[[2]]$res$sdev 

percent_variation <- sdev^2 / sum(sdev^2)

tibble(component = unique(tidied_pca$component),
       percent_var = percent_variation) %>%
  filter(component %in% paste0("PC", 1:20)) %>%
  mutate(component = fct_inorder(component)) %>%
  ggplot(aes(component, percent_var)) +
  geom_col(position = "stack") +
  geom_text(aes(label = scales::percent((round(percent_var,4))),
            vjust = -0.5), size = 3, color = "red"
            ) +
  geom_text(aes(label = scales::percent(cumsum(round(percent_var,4))),
            vjust = -2), size = 4
            ) +
  scale_y_continuous(labels = scales::percent_format(), limits = c(0,.35))
```
- The cummulative vairance is listed at the top of each bar.  The variance captured by each PC is below <br>
- The first 5 PCs capture 70% of the variance, the first 9 capture 80%, the first 16 capture 90%

Capturing 80% of the variance might be a good place to start

### Recipe that captures 80% variance
```{r}
nba_rec_80 <- recipe(~ ., data = model_df) %>%
  update_role(Year, Player, Pos, Tm, new_role = "id") %>%
  step_normalize(all_predictors()) %>%
  step_pca(all_predictors(), num_comp = 9)

nba_prep_80 <- prep(nba_rec_80)
```

Start with 3 clusters just to see how the data looks
```{r}
# the template field stores the PCA variables
pca_80 <- nba_prep_80$template %>%
    mutate(Year = as.character(Year),
         Player = as.character(Player),
         Pos = as.character(Pos),
         Tm = as.character(Tm)) %>%
  as_tibble()

set.seed(525)

#start by looking at 3 clusters
kclust <- kmeans(pca_80 %>% select(-c(1:4)), centers = 3)
```

### Tidy the kclust data
```{r}
#The tidy function gives us the centers from each variable (PCs)
tidy(kclust)
```

### Augment and plot the kclust data
```{r}
#augment shows all the data and includes the cluster number
head(augment(kclust, pca_80),5) 
```

```{r}
augment(kclust, pca_80) %>%
  ggplot(aes(PC1, PC2, color = .cluster)) + # the PCx values can be changed as needed
  geom_point()
```

### Glance the kclust data
```{r}
#glance gives us other useful metrics from the model
glance(kclust)
```

### Elbow Plot: Finding the optimal number of k
```{r fig.width = 10, fig.height=3}
kclusts <- 
  tibble(k = 1:15) %>%
  mutate(
    kclust = map(k, ~ kmeans(pca_80 %>% select(-c(1:4)), .x)),
    tidied = map(kclust, tidy),
    glanced = map(kclust, glance),
    augmented = map(kclust, augment, pca_80 %>% select(-c(1:4)))
  )

kclusts %>%
  unnest(glanced) %>%
  ggplot(aes(k, tot.withinss)) +
  geom_line(alpha = 0.8) +
  geom_point(size = 2)

```

The elbow looks like it's at 3.  It may be beneficial to plot the change for each k from k to k+1, k+2, and k+3

### Elbow plots - k+1, k+2, k+3
This shows the difference from k to k+1, k to k+2, and k to k+3
```{r}
kclusts %>%
  unnest(glanced) %>%
  ggplot(aes(k, tot.withinss)) +

  #change from k = 1 to k = 2
  geom_line(aes(k, tot.withinss-lead(tot.withinss), color = "k+1"), alpha = 0.8 ) +
  geom_point(aes(k, tot.withinss-lead(tot.withinss)),size = 2,color = "red") +

  #change from k = 1 to k = 3
  geom_line(aes(k, tot.withinss-lead(tot.withinss,n=2),color = "k+2"), alpha = 0.8) +
  geom_point(aes(k, tot.withinss-lead(tot.withinss,n=2)),size = 2, color = "green") +

  #change from k = 1 to k = 4
  geom_line(aes(k, tot.withinss-lead(tot.withinss,n=3),color = "k+3"), alpha = 0.8) +
  geom_point(aes(k, tot.withinss-lead(tot.withinss,n=3)),size = 2, color = "blue") +
  
  labs(
    x = "Number of Clusters (k)",
    y = "tot.withinss", color = "",
    title = "Where does the data level off?"
  )
```
It's a bit difficult to determine the optimal number of k.  An argument could be made for 3 (similar to the above just using k), but it seems unreasonable to have just 3 positions for NBA players. It could be beneficial to have a team with players that all fall in the "best" cluster, but more analysis would have to be done on historically good teams.

An argument could be made for 6 or 8 when looking at the difference in tot.withinss from k to k+1/k+2/k+3 - it levels off around there.

Filtering down the data to include less players might make things a bit easier to interpret

### Try PCA with less data
From the initial cleaning, players with mpg of less than 12 minutes per game were removed.  Removing players with less than 20 mins per game could make things more clear.

```{r}
model_df_mpg20 <-  NBACleanData %>%
  filter(MP_pg >= 20)

nba_rec_mpg20 <- recipe(~ ., data = model_df_mpg20) %>%
  update_role(Year, Player, Pos, Tm, new_role = "id") %>%
  step_normalize(all_predictors()) %>%
  step_pca(all_predictors())

nba_prep_mpg20 <- prep(nba_rec_mpg20)

tidied_pca_mpg20 <- tidy(nba_prep_mpg20, 2)

```

### How much variance is being captured?
```{r, fig.width= 14, fig.height = 3}
sdev <- nba_prep_mpg20$steps[[2]]$res$sdev

percent_variation <- sdev^2 / sum(sdev^2)

tibble(component = unique(tidied_pca_mpg20$component),
       percent_var = percent_variation) %>%
  filter(component %in% paste0("PC", 1:20)) %>%
  mutate(component = fct_inorder(component)) %>%
  ggplot(aes(component, percent_var)) +
  geom_col(position = "stack") +
  geom_text(aes(label = scales::percent((round(percent_var,4))),
            vjust = -0.5), size = 3, color = "red"
            ) +
  geom_text(aes(label = scales::percent(cumsum(round(percent_var,4))),
            vjust = -2), size = 4
            ) +
  scale_y_continuous(labels = scales::percent_format(), limits = c(0,.35))
```

- The cummulative vairance is listed at the top of each bar.  The variance captured by each PC is below

- Initial method: The first 5 PCs capture 70% of the variance, the first 9 capture 80%, the first 16 capture 90%
    
- Reduced data Method: The first 5 PCs capture 70% of the variance, the first 8 capture 80%, the first 14 capture 90%
    
- More or less the same, it takes one less PC to capture 80% of the variance

I was hoping for a bit more of a change in the number of variables that accounted for 80% of the variance, but that doesn't appear to be the case

### Try PCA with less data part 2
There were 4 stat categories - Per Game, Per 100 possessions, Shooting, and Advanced.  Maybe running PCA on those will provide clearer results


#### Advanced
```{r}
model_df_adv <-  NBACleanData %>%
  select(Year, Player, Pos, Tm,contains("_adv"))

nba_rec_adv <- recipe(~ ., data = model_df_adv) %>%
  update_role(Year, Player, Pos, Tm, new_role = "id") %>%
  step_normalize(all_predictors()) %>%
  step_pca(all_predictors())

nba_prep_adv <- prep(nba_rec_adv)

tidied_pca_adv <- tidy(nba_prep_adv, 2)

sdev <- nba_prep_adv$steps[[2]]$res$sdev

percent_variation <- sdev^2 / sum(sdev^2)

tibble(component = unique(tidied_pca_adv$component),
       percent_var = percent_variation) %>%
  filter(component %in% paste0("PC", 1:10)) %>%
  mutate(component = fct_inorder(component)) %>%
  ggplot(aes(component, percent_var)) +
  geom_col(position = "stack") +
  geom_text(aes(label = scales::percent((round(percent_var,4))),
            vjust = -0.5), size = 4, color = "red"
            ) +
  geom_text(aes(label = scales::percent(cumsum(round(percent_var,4))),
            vjust = -2), size = 5
            ) +
  scale_y_continuous(labels = scales::percent_format(), limits = c(0,.50))
```
3 PCs account for 70%, 4 PCs 80%, 7 PCs 90%

#### Per 100 possessions
```{r}
model_df_pp <-  NBACleanData %>%
  select(Year, Player, Pos, Tm,contains("_pp"))

nba_rec_ppp <- recipe(~ ., data = model_df_pp) %>%
  update_role(Year, Player, Pos, Tm, new_role = "id") %>%
  step_normalize(all_predictors()) %>%
  step_pca(all_predictors())

nba_prep_pp <- prep(nba_rec_ppp)

tidied_pca_pp <- tidy(nba_prep_pp, 2)

sdev <- nba_prep_pp$steps[[2]]$res$sdev

percent_variation <- sdev^2 / sum(sdev^2)

tibble(component = unique(tidied_pca_pp$component),
       percent_var = percent_variation) %>%
  filter(component %in% paste0("PC", 1:10)) %>%
  mutate(component = fct_inorder(component)) %>%
  ggplot(aes(component, percent_var)) +
  geom_col(position = "stack") +
  geom_text(aes(label = scales::percent((round(percent_var,4))),
            vjust = -0.5), size = 4, color = "red"
            ) +
  geom_text(aes(label = scales::percent(cumsum(round(percent_var,4))),
            vjust = -2), size = 5
            ) +
  scale_y_continuous(labels = scales::percent_format(), limits = c(0,.35))
```
4 PCs account for 70%, 6 PCs 80%,9 PCs 90%

#### Shooting
```{r}
model_df_sho <-  NBACleanData %>%
  select(Year, Player, Pos, Tm,contains("_sho"))

nba_rec_sho <- recipe(~ ., data = model_df_sho) %>%
  update_role(Year, Player, Pos, Tm, new_role = "id") %>%
  step_normalize(all_predictors()) %>%
  step_pca(all_predictors())

nba_prep_sho <- prep(nba_rec_sho)

tidied_pca_psho <- tidy(nba_prep_sho, 2)

sdev <- nba_prep_sho$steps[[2]]$res$sdev

percent_variation <- sdev^2 / sum(sdev^2)

tibble(component = unique(tidied_pca_psho$component),
       percent_var = percent_variation) %>%
  filter(component %in% paste0("PC", 1:10)) %>%
  mutate(component = fct_inorder(component)) %>%
  ggplot(aes(component, percent_var)) +
  geom_col(position = "stack") +
  geom_text(aes(label = scales::percent((round(percent_var,4))),
            vjust = -0.5), size = 3, color = "red"
            ) +
  geom_text(aes(label = scales::percent(cumsum(round(percent_var,4))),
            vjust = -2), size = 4
            ) +
    scale_y_continuous(labels = scales::percent_format(), limits = c(0,.35))
```
5 PCs account for 70%, 7 PCs 80%,9 PCs 90%

#### Per Game
```{r}
model_df_pg <-  NBACleanData %>%
  select(Year, Player, Pos, Tm,contains("_pg"))

nba_rec_pg <- recipe(~ ., data = model_df_pg) %>%
  update_role(Year, Player, Pos, Tm, new_role = "id") %>%
  step_normalize(all_predictors()) %>%
  step_pca(all_predictors())

nba_prep_pg <- prep(nba_rec_pg)

tidied_pca_pg <- tidy(nba_prep_pg, 2)

sdev <- nba_prep_pg$steps[[2]]$res$sdev

percent_variation <- sdev^2 / sum(sdev^2)

tibble(component = unique(tidied_pca_pg$component),
       percent_var = percent_variation) %>%
  filter(component %in% paste0("PC", 1:10)) %>%
  mutate(component = fct_inorder(component)) %>%
  ggplot(aes(component, percent_var)) +
  geom_col(position = "stack") +
  geom_text(aes(label = scales::percent((round(percent_var,4))),
            vjust = -0.5), size = 3, color = "red"
            ) +
  geom_text(aes(label = scales::percent(cumsum(round(percent_var,4))),
            vjust = -2), size = 4
            ) +
    scale_y_continuous(labels = scales::percent_format(), limits = c(0,.35))
```
2 PCs account for 70%, 3 PCs 80%,6 PCs 90%

Per game stats account for the most variance with the least amount of data.  We'll take a look at kmeans for only per game statistics

### Try PCA with less data part 3 - only per game statistics
```{r}
# the template stores the PCA variables
pca_pg <- nba_prep_pg$template %>%
    mutate(Year = as.character(Year),
         Player = as.character(Player),
         Pos = as.character(Pos),
         Tm = as.character(Tm)) %>%
  as_tibble()

set.seed(525)

kclust <- kmeans(pca_pg %>% select(-c(1:4)), centers = 3, nstart = 1000, iter.max = 10000)

```

```{r}
kclusts <- 
  tibble(k = 1:15) %>%
  mutate(
    kclust = map(k, ~ kmeans(pca_pg %>% select(-c(1:4)), .x)),
    tidied = map(kclust, tidy),
    glanced = map(kclust, glance),
    augmented = map(kclust, augment, pca_pg %>% select(-c(1:4)))
  )

kclusts %>%
  unnest(glanced) %>%
  ggplot(aes(k, tot.withinss)) +
  geom_line(alpha = 0.8) +
  geom_point(size = 2)

```
The elbow looks like it's at 2. It may be beneficial to plot the change for each k from k to k+1, k+2, and k+3

### Elbow plots - k+1, k+2, k+3
This shows the difference from k to k+1, k to k+2, and k to k+3
```{r}
kclusts %>%
  unnest(glanced) %>%
  ggplot(aes(k, tot.withinss)) +

  #change from k = 1 to k = 2
  geom_line(aes(k, tot.withinss-lead(tot.withinss), color = "k+1"), alpha = 0.8 ) +
  geom_point(aes(k, tot.withinss-lead(tot.withinss)),size = 2,color = "red") +

  #change from k = 1 to k = 3
  geom_line(aes(k, tot.withinss-lead(tot.withinss,n=2),color = "k+2"), alpha = 0.8) +
  geom_point(aes(k, tot.withinss-lead(tot.withinss,n=2)),size = 2, color = "green") +

  #change from k = 1 to k = 4
  geom_line(aes(k, tot.withinss-lead(tot.withinss,n=3),color = "k+3"), alpha = 0.8) +
  geom_point(aes(k, tot.withinss-lead(tot.withinss,n=3)),size = 2, color = "blue") +
  
  labs(
    x = "Number of Clusters (k)",
    y = "tot.withinss", color = "",
    title = "Where does the data level off?"
  )
```


# Hierarchical

I'll use PCA for hierarchical clustering as well to reduce the dimensionality of the data

## All Data
This plot is the same as the intial plot using all the data, just as a reminder
```{r fig.height = 6, fig.width = 16}
# recipe
nba_rec_hier <- recipe(~ ., data = model_df) %>%
  update_role(Year, Player, Pos, Tm, new_role = "id") %>%
  step_normalize(all_predictors()) %>%
  step_pca(all_predictors(), num_comp = 20)

#prep -  needed for tidied_pca and plot
nba_prep_hier <- prep(nba_rec_hier)

tidied_pca_hier <- tidy(nba_prep_hier,2)

sdev <- nba_prep_hier$steps[[2]]$res$sdev

percent_variation <- sdev^2 / sum(sdev^2)

tibble(component = unique(tidied_pca_hier$component),
       percent_var = percent_variation) %>%
  filter(component %in% paste0("PC", 1:20)) %>%
  mutate(component = fct_inorder(component)) %>%
  ggplot(aes(component, percent_var)) +
  geom_col(position = "stack") +
  geom_text(aes(label = scales::percent((round(percent_var,4))),
            vjust = -0.5), size = 3, color = "red"
            ) +
  geom_text(aes(label = scales::percent(cumsum(round(percent_var,4))),
            vjust = -2), size = 3.5
            ) +
  scale_y_continuous(labels = scales::percent_format(), limits = c(0,.35))
```

## Try the different hierarhcical clustering methods
```{r}
# new dataframe
nba_bake_hier <- bake(nba_prep_hier, new_data = NULL) %>%
  mutate(across(where(is.factor), as.character)) 

# the dendrograms take too long to generate if all the data is included, so a sample is taken
model_df_hc <- nba_bake_hier %>%
  sample_n(1000) %>%
  select(-c(1:4)) 

set.seed(42069)

# Distance between observations matrix
d <- dist(x=model_df_hc, method = "euclidean")

# Hierarchical clustering using Complete Linkage
nba_hclust_complete <- hclust(d, method = "complete")

# Hierarchical clustering using Average Linkage
nba_hclust_average <- hclust(d, method = "average")

# Hierarchical clustering using Ward Linkage
nba_hclust_ward <- hclust(d, method = "ward.D2")

```

#### Dendrogram of each
```{r}
library(factoextra)

fviz_dend(nba_hclust_complete, main = "Complete")
fviz_dend(nba_hclust_average, main = "Average Linkage")
fviz_dend(nba_hclust_ward, main = "Ward")

```

The below looks at the agglomerative coefficent, which measures clustering structure of the dataset.  A value closer to 1 means more balanced.

```{r}
library(cluster)

ac_metric <- list(
  complete_ac = agnes(model_df_hc, metric = "euclidean", method = "complete")$ac,
  average_ac = agnes(model_df_hc, metric = "euclidean", method = "average")$ac,
  ward_ac = agnes(model_df_hc, metric = "euclidean", method = "ward")$ac
)

ac_metric
```
Ward's is the closest to one, so that's the method that we'll use.

I can run with all the data and omit the dendrograms
```{r}
# # the dendrograms take too long to generate if all the data is included, so a sample is taken
# model_df_hc <- nba_bake_hier %>%
#   select(-c(1:4)) 
# 
# set.seed(42069)
# 
# # Distance between observations matrix
# d <- dist(x=model_df_hc, method = "euclidean")
# 
# # Hierarchical clustering using Complete Linkage
# nba_hclust_complete <- hclust(d, method = "complete")
# 
# # Hierarchical clustering using Average Linkage
# nba_hclust_average <- hclust(d, method = "average")
# 
# # Hierarchical clustering using Ward Linkage
# nba_hclust_ward <- hclust(d, method = "ward.D2")
# 
# ac_metric <- list(
#   complete_ac = agnes(model_df_hc, metric = "euclidean", method = "complete")$ac,
#   average_ac = agnes(model_df_hc, metric = "euclidean", method = "average")$ac,
#   ward_ac = agnes(model_df_hc, metric = "euclidean", method = "ward")$ac
# )
# 
# ac_metric
```

This further supports using wards

This will give us the optimal number of clusters.  Similar to the methods used earlier in the analysis with k-means
```{r}
# fviz_nbclust(model_df_hc, FUNcluster = hcut, method = "wss", k.max = 15)

```
Again, this gives us 3, even in a lower dimensional space.

